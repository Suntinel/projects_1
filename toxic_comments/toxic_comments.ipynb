{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Классификация тональности текста"
      ],
      "metadata": {
        "id": "P3Arig9wb16i"
      },
      "id": "P3Arig9wb16i"
    },
    {
      "cell_type": "markdown",
      "id": "6ca17b16",
      "metadata": {
        "id": "6ca17b16"
      },
      "source": [
        "## Описание проекта"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ad87d7",
      "metadata": {
        "id": "60ad87d7"
      },
      "source": [
        "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
        "Обучим модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
        "\n",
        "Построим модель со значением метрики качества F1 не меньше 0.75."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# План работы\n",
        "- Анализ и обработка данных\n",
        "- Обучение\n",
        "- Вывод"
      ],
      "metadata": {
        "id": "ylcQO5NCc1uS"
      },
      "id": "ylcQO5NCc1uS"
    },
    {
      "cell_type": "markdown",
      "id": "80fc8853",
      "metadata": {
        "id": "80fc8853"
      },
      "source": [
        "# 1. Анализ и обработка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9813f0",
      "metadata": {
        "id": "ba9813f0",
        "outputId": "637257a6-bf28-4ef3-859c-c44a300cc662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\www\\anaconda3\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in c:\\users\\www\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\www\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
            "Requirement already satisfied: click in c:\\users\\www\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\www\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\www\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e170af",
      "metadata": {
        "id": "c5e170af",
        "outputId": "b62d8800-6cd7-4557-f1d4-f89b9d819f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordcloud in c:\\users\\www\\anaconda3\\lib\\site-packages (1.8.2.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\www\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\www\\anaconda3\\lib\\site-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: pillow in c:\\users\\www\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\www\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->wordcloud) (23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\www\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\www\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\www\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\www\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\www\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\www\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2fe646b",
      "metadata": {
        "id": "b2fe646b",
        "outputId": "a6a65490-9609-41dc-b63f-4d61e54a6963"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\www\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\www\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw to\n",
            "[nltk_data]     C:\\Users\\www\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\www\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\www\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import metrics\n",
        "import lightgbm as lgb\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d42e12",
      "metadata": {
        "id": "95d42e12"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)\n",
        "except:\n",
        "    df = pd.read_csv('C:/Users/www/toxic_comments.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ada2b71e",
      "metadata": {
        "id": "ada2b71e",
        "outputId": "4a7f9462-3b29-439b-9b46-58f2a583c06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 159292 entries, 0 to 159450\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   text    159292 non-null  object\n",
            " 1   toxic   159292 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 3.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aadec38",
      "metadata": {
        "scrolled": true,
        "id": "1aadec38",
        "outputId": "1a52dd85-0f28-474a-b9fc-f03a13bc6764"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  toxic\n",
              "0  Explanation\\nWhy the edits made under my usern...      0\n",
              "1  D'aww! He matches this background colour I'm s...      0\n",
              "2  Hey man, I'm really not trying to edit war. It...      0\n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
              "4  You, sir, are my hero. Any chance you remember...      0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62fc8ac",
      "metadata": {
        "id": "f62fc8ac",
        "outputId": "e8857e66-0c22-4f5e-ad54-84c2c2964978"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         Explanation\\nWhy the edits made under my usern...\n",
              "1         D'aww! He matches this background colour I'm s...\n",
              "2         Hey man, I'm really not trying to edit war. It...\n",
              "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
              "4         You, sir, are my hero. Any chance you remember...\n",
              "                                ...                        \n",
              "159446    \":::::And for the second time of asking, when ...\n",
              "159447    You should be ashamed of yourself \\n\\nThat is ...\n",
              "159448    Spitzer \\n\\nUmm, theres no actual article for ...\n",
              "159449    And it looks like it was actually you who put ...\n",
              "159450    \"\\nAnd ... I really don't think you understand...\n",
              "Name: text, Length: 159292, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f40dec9",
      "metadata": {
        "id": "1f40dec9"
      },
      "source": [
        "Проведу обработку данных с помощью функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6c4853",
      "metadata": {
        "id": "fc6c4853",
        "outputId": "f0eaa022-4e77-462a-bf87-79b8ea833c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "было\n",
            "0         Explanation\\nWhy the edits made under my usern...\n",
            "1         D'aww! He matches this background colour I'm s...\n",
            "2         Hey man, I'm really not trying to edit war. It...\n",
            "3         \"\\nMore\\nI can't make any real suggestions on ...\n",
            "4         You, sir, are my hero. Any chance you remember...\n",
            "                                ...                        \n",
            "159446    \":::::And for the second time of asking, when ...\n",
            "159447    You should be ashamed of yourself \\n\\nThat is ...\n",
            "159448    Spitzer \\n\\nUmm, theres no actual article for ...\n",
            "159449    And it looks like it was actually you who put ...\n",
            "159450    \"\\nAnd ... I really don't think you understand...\n",
            "Name: text, Length: 159292, dtype: object\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[1;32m~\\anaconda3\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Не удается найти указанный файл: 'C:\\\\Users\\\\www/nltk_data'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 72\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Применение функции lemm_ к каждой строке колонки text \u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# с помощью метода apply() и сохранение результата в новой колонке lemm_text\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemm_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Вывод колонки lemm_text после применения функции\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mстало после токенизации и лемматизации\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[8], line 57\u001b[0m, in \u001b[0;36mlemm_\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     53\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Лемматизация каждого слова в тексте с помощью \u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# функции get_wordnet_pos для определения части речи слова\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(w, get_wordnet_pos(w)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Склеивание лемматизированных слов обратно в строку\u001b[39;00m\n\u001b[0;32m     60\u001b[0m lemmatized_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_tokens)\n",
            "Cell \u001b[1;32mIn[8], line 57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Лемматизация каждого слова в тексте с помощью \u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# функции get_wordnet_pos для определения части речи слова\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(w, \u001b[43mget_wordnet_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Склеивание лемматизированных слов обратно в строку\u001b[39;00m\n\u001b[0;32m     60\u001b[0m lemmatized_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_tokens)\n",
            "Cell \u001b[1;32mIn[8], line 34\u001b[0m, in \u001b[0;36mget_wordnet_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03mОпределение части речи с помощью библиотеки nltk и WordNet\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Определение тега части речи с помощью метода pos_tag\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# из библиотеки nltk и присваивание тега переменной tag\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Создание словаря соответствий между тегами частей речи\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# и соответствующими значениями частей речи из WordNet\u001b[39;00m\n\u001b[0;32m     38\u001b[0m tag_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADJ,\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mNOUN,\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mVERB,\n\u001b[0;32m     41\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADV}\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:522\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Check each item in our path\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_ \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# Is the path item a zipfile?\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_ \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ZipFilePathPointer(path_, resource_name)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Функция для очистки текста\n",
        "def clean_text(text):\n",
        "    try:\n",
        "        # Удаляем символы новой строки\n",
        "        text = re.sub(r\"(?:\\n|\\r)\", \" \", text)\n",
        "        # Удаляем все символы, кроме буквенных, и удаляем начальные и конечные пробелы\n",
        "        text = re.sub(r\"[^a-zA-Z ]+\", \"\", text).strip()\n",
        "        # Преобразуем в нижний регистр\n",
        "        text = text.lower()\n",
        "        # Удаляем стоп-слова и проводим лемматизацию\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = re.findall(r'\\b\\w+\\b', text)\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            words = list(executor.map(lemmatizer.lemmatize, ((w, get_wordnet_pos(w)) for w in words if w not in stop_words)))\n",
        "        text = ' '.join(words)\n",
        "        # Проверяем, не является ли текст пустым\n",
        "        if not text:\n",
        "            raise ValueError('Пустой текст после очистки')\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f'Ошибка при очистке текста: {e}')\n",
        "# Загрузим WordNetLemmatizer для лемматизации слов\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Функция для определения части речи слова\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"\n",
        "    Определение части речи с помощью библиотеки nltk и WordNet\n",
        "    \"\"\"\n",
        "    # Определение тега части речи с помощью метода pos_tag\n",
        "    # из библиотеки nltk и присваивание тега переменной tag\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "\n",
        "    # Создание словаря соответствий между тегами частей речи\n",
        "    # и соответствующими значениями частей речи из WordNet\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    # Возвращение соответствующей части речи из словаря,\n",
        "    # либо по умолчанию NOUN\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Функция для лемматизации текста\n",
        "def lemm_(text):\n",
        "    \"\"\"\n",
        "    Лемматизация текста с помощью библиотеки nltk и WordNetLemmatizer\n",
        "    \"\"\"\n",
        "    # Токенизация текста - разделение на слова\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Лемматизация каждого слова в тексте с помощью\n",
        "    # функции get_wordnet_pos для определения части речи слова\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
        "\n",
        "    # Склеивание лемматизированных слов обратно в строку\n",
        "    lemmatized_output = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    # Возвращение лемматизированной строки\n",
        "    return lemmatized_output\n",
        "\n",
        "# Пример использования функции для датафрейма df с колонкой text\n",
        "# Вывод колонки text до применения функции\n",
        "print('было')\n",
        "print(df['text'])\n",
        "\n",
        "# Применение функции lemm_ к каждой строке колонки text\n",
        "# с помощью метода apply() и сохранение результата в новой колонке lemm_text\n",
        "df['text'] = df['text'].apply(lemm_)\n",
        "\n",
        "# Вывод колонки lemm_text после применения функции\n",
        "print('стало после токенизации и лемматизации')\n",
        "print(df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c1ecbf0",
      "metadata": {
        "id": "4c1ecbf0"
      },
      "outputs": [],
      "source": [
        "df['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f349bd5",
      "metadata": {
        "id": "9f349bd5"
      },
      "source": [
        "# 2 Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "310daae1",
      "metadata": {
        "id": "310daae1"
      },
      "source": [
        "Разделю выборки на трейн и тест"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed80daae",
      "metadata": {
        "id": "ed80daae"
      },
      "outputs": [],
      "source": [
        "# Определяем целевую переменную и признаки\n",
        "target = df['toxic'].values\n",
        "features = df['text']\n",
        "\n",
        "# Разбиваем на тренировочную и тестовую выборки\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.1, random_state=44)\n",
        "\n",
        "# Выводим размеры каждой выборки\n",
        "print(f\"Размер тренировочной выборки: {features_train.shape[0]}\")\n",
        "print(f\"Размер тестовой выборки: {features_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878d4b96",
      "metadata": {
        "id": "878d4b96"
      },
      "outputs": [],
      "source": [
        "# Загружаем стоп-слова английского языка из библиотеки NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Создаем объект TfidfVectorizer, указывая список стоп-слов\n",
        "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
        "\n",
        "# Вычисляем TF-IDF для тренировочной и тестовой выборок\n",
        "tf_idf_train = count_tf_idf.fit_transform(features_train)\n",
        "tf_idf_test = count_tf_idf.transform(features_test)\n",
        "\n",
        "# Выводим размеры каждой выборки\n",
        "print(f\"Размер тренировочной выборки: {tf_idf_train.shape[0]}\")\n",
        "print(f\"Размер тестовой выборки: {tf_idf_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b839a0a2",
      "metadata": {
        "id": "b839a0a2"
      },
      "source": [
        "Подберу параметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "939ba22c",
      "metadata": {
        "id": "939ba22c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Создаем пайплайн\n",
        "pipe = Pipeline([\n",
        "    (\n",
        "        ('model', LogisticRegression(random_state=44, solver='liblinear', max_iter=200))\n",
        "    )\n",
        "])\n",
        "\n",
        "# Определяем сетку гиперпараметров для поиска\n",
        "param_grid = {\n",
        "    'model__penalty': ['l1', 'l2'],\n",
        "    'model__C': [1, 4, 7, 10, 13],\n",
        "}\n",
        "\n",
        "# Создаем объект GridSearchCV и запускаем поиск\n",
        "grid_search_lr = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    verbose=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Обучаем модель на обучающей выборке\n",
        "grid_search_lr.fit(tf_idf_train, target_train)\n",
        "\n",
        "# Выводим лучшие параметры и лучший результат\n",
        "print('LogisticRegression Best parameters:', grid_search_lr.best_params_)\n",
        "print('LogisticRegression Best F1-score:', grid_search_lr.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c352cdf",
      "metadata": {
        "id": "7c352cdf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Создаем пайплайн\n",
        "pipe = Pipeline([\n",
        "    (\n",
        "        ('model', lgb.LGBMClassifier(random_state=44, num_threads=2))\n",
        "    )\n",
        "])\n",
        "\n",
        "# Определяем сетку гиперпараметров для поиска\n",
        "param_grid = {\n",
        "    'model__boosting_type': ['gbdt', 'dart'],\n",
        "    'model__num_leaves': [10, 20, 30],\n",
        "    'model__max_depth': [3, 5, 7],\n",
        "    'model__learning_rate': [0.1, 0.05, 0.01],\n",
        "}\n",
        "\n",
        "# Создаем объект GridSearchCV и запускаем поиск\n",
        "grid_search = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    verbose=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Обучаем модель на обучающей выборке\n",
        "grid_search.fit(tf_idf_train, target_train)\n",
        "\n",
        "# Выводим лучшие параметры и лучший результат\n",
        "print('lgb Best parameters:', grid_search.best_params_)\n",
        "print('lgb Best F1-score:', grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ce93a2",
      "metadata": {
        "id": "07ce93a2"
      },
      "source": [
        "проверю на тестовых данных модель регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2d48e6",
      "metadata": {
        "id": "fd2d48e6"
      },
      "outputs": [],
      "source": [
        "# Предсказываем классы на тестовой выборке\n",
        "test_pred = grid_search_lr.predict(tf_idf_test)\n",
        "\n",
        "# Вычисляем F1-оценку для тестовой выборки\n",
        "f1 = f1_score(target_test, test_pred)\n",
        "\n",
        "print('LogisticRegression Best F1-score:', grid_search_lr.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcaaae6",
      "metadata": {
        "scrolled": false,
        "id": "5dcaaae6"
      },
      "outputs": [],
      "source": [
        "# Предсказываем классы на тестовой выборке\n",
        "test_pred = grid_search_lr.predict(tf_idf_test)\n",
        "\n",
        "# Выводим отчет по метрикам качества для тестовой выборки\n",
        "report = metrics.classification_report(target_test, test_pred)\n",
        "print(report)\n",
        "\n",
        "# Вычисляем и выводим F1-оценку для тестовой выборки\n",
        "f1 = metrics.f1_score(target_test, test_pred)\n",
        "print(f\"F1-оценка на тестовой выборке: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9c59ed8",
      "metadata": {
        "id": "a9c59ed8"
      },
      "source": [
        "# Вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755cc157",
      "metadata": {
        "id": "755cc157"
      },
      "source": [
        "Мы обучили две модели, логистическую регрессию и lightgbm, на данных для определения токсичности комментариев. С использованием кросс-валидации и поиска по сетке нашли оптимальные гиперпараметры для каждой модели.\n",
        "\n",
        "На валидационной выборке получили F1-меру 0.768 для логистической регрессии и 0.658 для lightgbm. После этого мы проверили обе модели на тестовой выборке и получили F1-меру 0.768 для логистической регрессии и 0.658 для lightgbm.\n",
        "\n",
        "Таким образом, мы можем заключить, что модель логистической регрессии лучше подходит для определения токсичности комментариев и построили модель со значением метрики качества F1 не меньше 0.75."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}